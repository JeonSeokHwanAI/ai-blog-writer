"""
ë„¤ì´ë²„ ë¸”ë¡œê·¸ ê¸°ë°˜ í‚¤ì›Œë“œ ìˆ˜ì§‘ ëª¨ë“ˆ

ë„¤ì´ë²„ ê²€ìƒ‰ APIë¥¼ í™œìš©í•˜ì—¬:
- í‚¤ì›Œë“œë³„ ë¬¸ì„œ ìˆ˜ ì¡°íšŒ
- ë¸”ë¡œê·¸ ì œëª©ì—ì„œ ì—°ê´€ í‚¤ì›Œë“œ ì¶”ì¶œ
- ìë™ì™„ì„± í‚¤ì›Œë“œ ìˆ˜ì§‘
- ê²½ìŸë„/ìµœì‹ ì„± ë¶„ì„
"""

import requests
import urllib.parse
import re
import json
import os
import time
from datetime import datetime, timedelta
from collections import Counter
from typing import List, Dict, Optional, Tuple

# ì„¤ì • íŒŒì¼ ê²½ë¡œ
CONFIG_FILE = "config/keyword_config.json"


def load_config() -> Dict:
    """ì„¤ì • íŒŒì¼ ë¡œë“œ"""
    if os.path.exists(CONFIG_FILE):
        try:
            with open(CONFIG_FILE, "r", encoding="utf-8") as f:
                return json.load(f)
        except:
            pass
    return {}


def save_config(config: Dict):
    """ì„¤ì • íŒŒì¼ ì €ì¥"""
    with open(CONFIG_FILE, "w", encoding="utf-8") as f:
        json.dump(config, f, indent=2, ensure_ascii=False)


class KeywordCollector:
    """ë¸”ë¡œê·¸ ê¸°ë°˜ í‚¤ì›Œë“œ ìˆ˜ì§‘ê¸° (ë„¤ì´ë²„ API ì‚¬ìš©)"""

    # ë¶ˆìš©ì–´ (Stop words)
    STOP_WORDS = {
        'ì˜', 'ê°€', 'ì´', 'ì€', 'ë“¤', 'ëŠ”', 'ì¢€', 'ì˜', 'ê±', 'ê³¼', 'ë„', 'ë¥¼', 'ìœ¼ë¡œ', 'ì', 'ì—', 'ì™€', 'í•œ', 'í•˜ë‹¤',
        'ë°', 'ê·¸', 'ì €', 'ê²ƒ', 'ìˆ˜', 'ë“±', 'ë”', 'ìœ„', 'ì¤‘', 'ë¡œ', 'ë§Œ', 'ìˆë‹¤', 'ì—†ë‹¤', 'í•˜ëŠ”', 'ë˜ëŠ”',
        'the', 'a', 'an', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had',
        'ìˆëŠ”', 'ì—†ëŠ”', 'í•˜ê³ ', 'ë˜ê³ ', 'ëœ', 'í• ', 'ë ', 'ì¸', 'ì ', 'ë‹¤ìŒ', 'ì—ì„œ', 'ê¹Œì§€', 'ë¶€í„°', 'ìœ¼ë¡œì„œ'
    }

    # ê²€ìƒ‰ ì˜ë„ íŒ¨í„´
    INTENT_PATTERNS = {
        "ì •ë³´/í”„ë¡œí•„": ["í”„ë¡œí•„", "ëˆ„êµ¬", "ë‚˜ì´", "í•™ë ¥", "ì•½ë ¥", "ê²½ë ¥", "ì¸ë¬¼"],
        "í›„ê¸°/ë¦¬ë·°": ["í›„ê¸°", "ë¦¬ë·°", "ì†”ì§", "ì‹¤ì œ", "ì‚¬ìš©", "ì²´í—˜", "ë°©ë¬¸"],
        "ë°©ë²•/ê°€ì´ë“œ": ["ë°©ë²•", "í•˜ëŠ”ë²•", "ë§Œë“¤ê¸°", "ì„¤ì¹˜", "ì‚¬ìš©ë²•", "ê°€ì´ë“œ", "íŒ"],
        "ì¶”ì²œ/ë¹„êµ": ["ì¶”ì²œ", "ë¹„êµ", "ìˆœìœ„", "best", "top", "ë² ìŠ¤íŠ¸", "ì¸ê¸°"],
        "ë‰´ìŠ¤/ì´ìŠˆ": ["ë¼ë©°", "ì‚¬ê±´", "ë‰´ìŠ¤", "ì†ë³´", "ë°œí‘œ", "ê²°ì •", "íŒê²°"],
        "ê°€ê²©/ë¹„ìš©": ["ê°€ê²©", "ë¹„ìš©", "ì–¼ë§ˆ", "í• ì¸", "ì„¸ì¼", "ë¬´ë£Œ", "ìœ ë£Œ"],
        "ì—¬í–‰/ë§›ì§‘": ["ë§›ì§‘", "ì—¬í–‰", "ì½”ìŠ¤", "ì¼ì •", "ì˜ˆì•½", "ê´€ê´‘", "í¬ì¸"],
        "ê°ìƒ/ë¶„ì„": ["ì¤„ê±°ë¦¬", "ê²°ë§", "í•´ì„", "ë¶„ì„", "ì •ë¦¬", "ìš”ì•½", "ìŠ¤í¬"]
    }

    def __init__(self, client_id: str = None, client_secret: str = None):
        """
        Args:
            client_id: ë„¤ì´ë²„ ê²€ìƒ‰ API Client ID
            client_secret: ë„¤ì´ë²„ ê²€ìƒ‰ API Client Secret
        """
        # ì„¤ì • íŒŒì¼ì—ì„œ ë¡œë“œ
        if client_id is None or client_secret is None:
            config = load_config()
            client_id = client_id or config.get("NAVER_BLOG_CLIENT_ID", "")
            client_secret = client_secret or config.get("NAVER_BLOG_CLIENT_SECRET", "")

        self.client_id = client_id
        self.client_secret = client_secret
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        }

    def _get_api_headers(self) -> dict:
        """API ìš”ì²­ í—¤ë” ìƒì„±"""
        return {
            "X-Naver-Client-Id": self.client_id,
            "X-Naver-Client-Secret": self.client_secret
        }

    def is_configured(self) -> bool:
        """API í‚¤ê°€ ì„¤ì •ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸"""
        return bool(self.client_id and self.client_secret)

    def get_document_count(self, keyword: str) -> int:
        """
        í‚¤ì›Œë“œì˜ ë¸”ë¡œê·¸ ë¬¸ì„œ ìˆ˜ ì¡°íšŒ

        Args:
            keyword: ê²€ìƒ‰í•  í‚¤ì›Œë“œ

        Returns:
            ë¸”ë¡œê·¸ ë¬¸ì„œ ì´ ê°œìˆ˜
        """
        if not self.is_configured():
            print("[ì˜¤ë¥˜] API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return 0

        try:
            enc_text = urllib.parse.quote(keyword)
            url = f"https://openapi.naver.com/v1/search/blog?query={enc_text}&display=1"

            response = requests.get(url, headers=self._get_api_headers(), timeout=5)
            response.raise_for_status()
            data = response.json()
            return data.get("total", 0)
        except Exception as e:
            print(f"[ì˜¤ë¥˜] ë¬¸ì„œ ìˆ˜ ì¡°íšŒ ì‹¤íŒ¨ ({keyword}): {e}")
            return 0

    def get_blog_titles(self, keyword: str, display: int = 50) -> List[Dict]:
        """
        ë¸”ë¡œê·¸ ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ì œëª©ê³¼ ë©”íƒ€ ì •ë³´ ê°€ì ¸ì˜¤ê¸°

        Args:
            keyword: ê²€ìƒ‰í•  í‚¤ì›Œë“œ
            display: ê°€ì ¸ì˜¬ ê²°ê³¼ ìˆ˜ (ìµœëŒ€ 100)

        Returns:
            [{"title": "...", "blogger": "...", "date": "...", "link": "..."}, ...]
        """
        if not self.is_configured():
            return []

        try:
            enc_text = urllib.parse.quote(keyword)
            url = f"https://openapi.naver.com/v1/search/blog?query={enc_text}&display={display}&sort=sim"

            response = requests.get(url, headers=self._get_api_headers(), timeout=10)
            response.raise_for_status()
            data = response.json()

            results = []
            for item in data.get("items", []):
                title = item.get("title", "").replace("<b>", "").replace("</b>", "")
                results.append({
                    "title": title,
                    "blogger": item.get("bloggername", ""),
                    "date": item.get("postdate", ""),
                    "link": item.get("link", ""),
                    "description": item.get("description", "").replace("<b>", "").replace("</b>", "")
                })
            return results
        except Exception as e:
            print(f"[ì˜¤ë¥˜] ë¸”ë¡œê·¸ ì œëª© ì¡°íšŒ ì‹¤íŒ¨ ({keyword}): {e}")
            return []

    def get_autocomplete_keywords(self, keyword: str, limit: int = 10) -> List[str]:
        """
        ë„¤ì´ë²„ ìë™ì™„ì„±ì—ì„œ ì—°ê´€ í‚¤ì›Œë“œ ê°€ì ¸ì˜¤ê¸° (API í‚¤ ë¶ˆí•„ìš”)

        Args:
            keyword: ê¸°ë³¸ í‚¤ì›Œë“œ
            limit: ìµœëŒ€ ê²°ê³¼ ìˆ˜

        Returns:
            ìë™ì™„ì„± í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
        """
        try:
            enc_text = urllib.parse.quote(keyword)
            url = f"https://ac.search.naver.com/nx/ac?q={enc_text}&con=1&frm=nv&ans=2&r_format=json&r_enc=UTF-8&r_unicode=0&t_koreng=1&run=2&rev=4&q_enc=UTF-8"

            response = requests.get(url, headers=self.headers, timeout=5)
            response.raise_for_status()
            data = response.json()

            suggestions = []
            items = data.get("items", [])
            if items and len(items) > 0:
                for item in items[0]:
                    if isinstance(item, list) and len(item) > 0:
                        suggestions.append(item[0])

            return suggestions[:limit]
        except Exception as e:
            print(f"[ì˜¤ë¥˜] ìë™ì™„ì„± ì¡°íšŒ ì‹¤íŒ¨ ({keyword}): {e}")
            return []

    def extract_keywords_from_titles(self, base_keyword: str, titles: List[Dict]) -> List[str]:
        """
        ë¸”ë¡œê·¸ ì œëª©ì—ì„œ ì—°ê´€ í‚¤ì›Œë“œ ì¶”ì¶œ

        Args:
            base_keyword: ê¸°ë³¸ í‚¤ì›Œë“œ
            titles: get_blog_titles() ê²°ê³¼

        Returns:
            "ê¸°ë³¸í‚¤ì›Œë“œ + ì¶”ì¶œë‹¨ì–´" ì¡°í•© ë¦¬ìŠ¤íŠ¸
        """
        base_words = set(re.split(r'[,\sÂ·|:\-\[\]()]+', base_keyword.lower()))
        base_words = {w.strip() for w in base_words if len(w.strip()) >= 2}

        word_counter = Counter()

        for item in titles:
            title = item.get("title", "")
            title_clean = title.replace(base_keyword, '')
            words = re.split(r'[,\sÂ·|:\-\[\]()ã€Œã€ã€ã€ã€ã€‘]+', title_clean)

            for word in words:
                word = word.strip()
                if len(word) >= 2 and word not in self.STOP_WORDS and word.lower() not in base_words:
                    word_counter[word] += 1

        extracted = []
        for word, count in word_counter.most_common(30):
            if count >= 2 and len(extracted) < 15:
                combined = f"{base_keyword} {word}"
                extracted.append(combined)

        return extracted

    def get_recent_blog_count(self, keyword: str, days: int = 30) -> int:
        """
        ìµœê·¼ Nì¼ê°„ ë°œí–‰ëœ ë¸”ë¡œê·¸ ê¸€ ìˆ˜ ì¡°íšŒ (ê²½ìŸë„ íŒë‹¨ìš©)

        Args:
            keyword: ê²€ìƒ‰í•  í‚¤ì›Œë“œ
            days: ìµœê·¼ ë©°ì¹ ê°„

        Returns:
            ìµœê·¼ ë°œí–‰ëœ ë¸”ë¡œê·¸ ê¸€ ìˆ˜
        """
        if not self.is_configured():
            return 0

        try:
            enc_text = urllib.parse.quote(keyword)
            url = f"https://openapi.naver.com/v1/search/blog?query={enc_text}&display=100&sort=date"

            response = requests.get(url, headers=self._get_api_headers(), timeout=10)
            response.raise_for_status()
            data = response.json()

            cutoff_date = datetime.now() - timedelta(days=days)
            recent_count = 0

            for item in data.get("items", []):
                postdate = item.get("postdate", "")
                if postdate and len(postdate) == 8:
                    try:
                        post_dt = datetime.strptime(postdate, "%Y%m%d")
                        if post_dt >= cutoff_date:
                            recent_count += 1
                    except:
                        pass

            return recent_count
        except Exception as e:
            print(f"[ì˜¤ë¥˜] ìµœê·¼ ë¸”ë¡œê·¸ ìˆ˜ ì¡°íšŒ ì‹¤íŒ¨ ({keyword}): {e}")
            return 0

    def get_news_count(self, keyword: str, days: int = 7) -> Dict:
        """
        ìµœê·¼ Nì¼ê°„ ë‰´ìŠ¤ ê¸°ì‚¬ ìˆ˜ ì¡°íšŒ (ìµœì‹ ì„± í™•ì¸)

        Args:
            keyword: ê²€ìƒ‰í•  í‚¤ì›Œë“œ
            days: ìµœê·¼ ë©°ì¹ ê°„

        Returns:
            {"total": ì „ì²´ìˆ˜, "recent": ìµœê·¼ìˆ˜}
        """
        if not self.is_configured():
            return {"total": 0, "recent": 0}

        try:
            enc_text = urllib.parse.quote(keyword)
            url = f"https://openapi.naver.com/v1/search/news?query={enc_text}&display=100&sort=date"

            response = requests.get(url, headers=self._get_api_headers(), timeout=10)
            response.raise_for_status()
            data = response.json()

            total = data.get("total", 0)
            cutoff_date = datetime.now() - timedelta(days=days)
            recent_news = 0

            for item in data.get("items", []):
                pubdate = item.get("pubDate", "")
                if pubdate:
                    try:
                        from email.utils import parsedate_to_datetime
                        pub_dt = parsedate_to_datetime(pubdate)
                        pub_dt = pub_dt.replace(tzinfo=None)
                        if pub_dt >= cutoff_date:
                            recent_news += 1
                    except:
                        pass

            return {"total": total, "recent": recent_news}
        except Exception as e:
            print(f"[ì˜¤ë¥˜] ë‰´ìŠ¤ ìˆ˜ ì¡°íšŒ ì‹¤íŒ¨ ({keyword}): {e}")
            return {"total": 0, "recent": 0}

    def analyze_search_intent(self, keyword: str, titles: List[Dict]) -> List[Tuple[str, int]]:
        """
        ë¸”ë¡œê·¸ ì œëª©ì—ì„œ ê²€ìƒ‰ ì˜ë„ ë¶„ì„

        Args:
            keyword: ê¸°ë³¸ í‚¤ì›Œë“œ
            titles: get_blog_titles() ê²°ê³¼

        Returns:
            [(ì˜ë„, íšŸìˆ˜), ...] ìƒìœ„ 3ê°œ
        """
        title_text = ' '.join([t.get("title", "") for t in titles]).lower()

        intent_counts = {k: 0 for k in self.INTENT_PATTERNS.keys()}

        for intent, patterns in self.INTENT_PATTERNS.items():
            for pattern in patterns:
                if pattern in title_text:
                    intent_counts[intent] += title_text.count(pattern)

        sorted_intents = sorted(intent_counts.items(), key=lambda x: x[1], reverse=True)
        return [(k, v) for k, v in sorted_intents if v > 0][:3]

    def analyze_keyword(self, keyword: str, analyze_competition: bool = True,
                       analyze_recency: bool = True, analyze_intent: bool = True) -> Dict:
        """
        í‚¤ì›Œë“œ ì¢…í•© ë¶„ì„

        Args:
            keyword: ë¶„ì„í•  í‚¤ì›Œë“œ
            analyze_competition: ê²½ìŸë„ ë¶„ì„ ì—¬ë¶€
            analyze_recency: ìµœì‹ ì„± ë¶„ì„ ì—¬ë¶€
            analyze_intent: ê²€ìƒ‰ ì˜ë„ ë¶„ì„ ì—¬ë¶€

        Returns:
            ì¢…í•© ë¶„ì„ ê²°ê³¼
        """
        result = {
            "keyword": keyword,
            "docs": 0,
            "is_golden": False,
            "rating": "",
            "competition": {},
            "recency": {},
            "intent": [],
            "related_keywords": []
        }

        # ë¬¸ì„œ ìˆ˜ ì¡°íšŒ
        result["docs"] = self.get_document_count(keyword)
        time.sleep(0.1)

        # ê³¨ë“  í‚¤ì›Œë“œ íŒë³„
        docs = result["docs"]
        if docs < 5000:
            result["is_golden"] = True
            result["rating"] = "â­â­â­ ë§¤ìš° ì¢‹ìŒ (ì €ê²½ìŸ ë¸”ë£¨ì˜¤ì…˜)"
        elif docs < 10000:
            result["is_golden"] = True
            result["rating"] = "â­â­ ì¢‹ìŒ (ì§„ì… ìš©ì´)"
        elif docs < 20000:
            result["rating"] = "â­ ë³´í†µ (ì ì • ê²½ìŸ)"
        else:
            result["rating"] = "ê²½ìŸ ìˆìŒ (ë ˆë“œì˜¤ì…˜)"

        # ë¸”ë¡œê·¸ ì œëª© ìˆ˜ì§‘ (ì—°ê´€ í‚¤ì›Œë“œ ë° ì˜ë„ ë¶„ì„ìš©)
        titles = self.get_blog_titles(keyword, display=50)
        time.sleep(0.1)

        # ê²½ìŸë„ ë¶„ì„
        if analyze_competition:
            recent_blogs = self.get_recent_blog_count(keyword, days=30)
            if recent_blogs < 10:
                comp_rating = "ğŸŸ¢ ë§¤ìš° ë‚®ìŒ (ë°”ë¡œ ì§„ì…!)"
            elif recent_blogs < 30:
                comp_rating = "ğŸŸ¡ ë‚®ìŒ (ì§„ì… ê°€ëŠ¥)"
            elif recent_blogs < 50:
                comp_rating = "ğŸŸ  ë³´í†µ (ì„¸ë¶€ í‚¤ì›Œë“œ ê³ ë ¤)"
            else:
                comp_rating = "ğŸ”´ ë†’ìŒ (ì„¸ë¶€ í‚¤ì›Œë“œ í•„ìš”)"

            result["competition"] = {
                "recent_30days": recent_blogs,
                "rating": comp_rating
            }
            time.sleep(0.1)

        # ìµœì‹ ì„± ë¶„ì„
        if analyze_recency:
            news_data = self.get_news_count(keyword, days=7)
            if news_data["recent"] > 10:
                news_rating = "ğŸ”¥ í•«ì´ìŠˆ (ì‹ ì†íˆ ì‘ì„±!)"
            elif news_data["recent"] > 5:
                news_rating = "ğŸ“° ì´ìŠˆ ìˆìŒ (ë¹ ë¥´ê²Œ ì‘ì„±)"
            elif news_data["recent"] > 0:
                news_rating = "ğŸ“ ì•½ê°„ì˜ ë‰´ìŠ¤ (ì‹¬ì¸µ ë¶„ì„ê¸€)"
            else:
                news_rating = "ğŸ“„ ì´ìŠˆ ì—†ìŒ (ì‹¬ì¸µ ë¶„ì„ê¸€ ì¶”ì²œ)"

            result["recency"] = {
                "news_total": news_data["total"],
                "news_recent_7days": news_data["recent"],
                "rating": news_rating
            }
            time.sleep(0.1)

        # ê²€ìƒ‰ ì˜ë„ ë¶„ì„
        if analyze_intent and titles:
            result["intent"] = self.analyze_search_intent(keyword, titles)

        # ì—°ê´€ í‚¤ì›Œë“œ ì¶”ì¶œ
        if titles:
            extracted = self.extract_keywords_from_titles(keyword, titles)
            autocomplete = self.get_autocomplete_keywords(keyword)

            all_related = []
            seen = set()
            for kw in extracted + autocomplete:
                if kw.lower() not in seen:
                    seen.add(kw.lower())
                    all_related.append(kw)

            result["related_keywords"] = all_related[:20]

        return result

    def collect_keywords(self, seed_keywords: List[str], max_keywords: int = 100,
                        golden_threshold: int = 10000) -> List[Dict]:
        """
        ì‹œë“œ í‚¤ì›Œë“œì—ì„œ ì—°ê´€ í‚¤ì›Œë“œê¹Œì§€ ìˆ˜ì§‘

        Args:
            seed_keywords: ì‹œì‘ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
            max_keywords: ìµœëŒ€ ìˆ˜ì§‘í•  í‚¤ì›Œë“œ ìˆ˜
            golden_threshold: ê³¨ë“  í‚¤ì›Œë“œ ê¸°ì¤€ ë¬¸ì„œ ìˆ˜

        Returns:
            ìˆ˜ì§‘ëœ í‚¤ì›Œë“œ ë¶„ì„ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        all_keywords = list(seed_keywords)
        searched = set()
        results = []

        print(f"[í‚¤ì›Œë“œ ìˆ˜ì§‘] ì‹œë“œ í‚¤ì›Œë“œ {len(seed_keywords)}ê°œì—ì„œ ì‹œì‘...")

        for seed in seed_keywords:
            if seed in searched:
                continue
            searched.add(seed)

            print(f"  â†’ '{seed}' ì—°ê´€ í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘...")

            titles = self.get_blog_titles(seed, display=50)
            if titles:
                extracted = self.extract_keywords_from_titles(seed, titles)
                for kw in extracted:
                    if kw not in all_keywords:
                        all_keywords.append(kw)

            autocomplete = self.get_autocomplete_keywords(seed)
            for kw in autocomplete:
                if kw not in all_keywords:
                    all_keywords.append(kw)

            time.sleep(0.2)

        print(f"[í‚¤ì›Œë“œ ìˆ˜ì§‘] ì´ {len(all_keywords)}ê°œ í‚¤ì›Œë“œ ë°œê²¬, ë¶„ì„ ì‹œì‘...")

        for i, keyword in enumerate(all_keywords[:max_keywords]):
            if keyword in searched and keyword not in seed_keywords:
                continue

            print(f"  [{i+1}/{min(len(all_keywords), max_keywords)}] '{keyword}' ë¶„ì„ ì¤‘...")

            docs = self.get_document_count(keyword)

            is_golden = docs <= golden_threshold
            if docs < 5000:
                rating = "â­â­â­ ë§¤ìš° ì¢‹ìŒ"
            elif docs < 10000:
                rating = "â­â­ ì¢‹ìŒ"
            elif docs < 20000:
                rating = "â­ ë³´í†µ"
            else:
                rating = "ê²½ìŸ ìˆìŒ"

            results.append({
                "keyword": keyword,
                "docs": docs,
                "is_golden": is_golden,
                "rating": rating
            })

            time.sleep(0.1)

        results.sort(key=lambda x: x["docs"])

        golden_count = sum(1 for r in results if r["is_golden"])
        print(f"\n[ì™„ë£Œ] ì´ {len(results)}ê°œ í‚¤ì›Œë“œ ë¶„ì„, ê³¨ë“  í‚¤ì›Œë“œ {golden_count}ê°œ")

        return results


def save_results(results: List[Dict], keyword: str, output_dir: str = "output"):
    """ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
    os.makedirs(output_dir, exist_ok=True)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    safe_keyword = re.sub(r'[<>:"/\\|?*]', '', keyword).strip()[:20]
    filename = f"{output_dir}/keywords_{safe_keyword}_{timestamp}.json"

    output = {
        "seed_keyword": keyword,
        "collected_at": datetime.now().isoformat(),
        "total_keywords": len(results),
        "golden_count": sum(1 for r in results if r.get("is_golden")),
        "keywords": results
    }

    with open(filename, "w", encoding="utf-8") as f:
        json.dump(output, f, indent=2, ensure_ascii=False)

    print(f"[ì €ì¥ ì™„ë£Œ] {filename}")
    return filename


# CLI ì‹¤í–‰
if __name__ == "__main__":
    import sys

    config = load_config()
    collector = KeywordCollector()

    if not collector.is_configured():
        print("=" * 50)
        print("ë„¤ì´ë²„ ê²€ìƒ‰ API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        print("=" * 50)
        print("\n1. https://developers.naver.com ì—ì„œ ì• í”Œë¦¬ì¼€ì´ì…˜ ë“±ë¡")
        print("2. ê²€ìƒ‰ API ì‚¬ìš© ì‹ ì²­")
        print("3. Client IDì™€ Client Secret ë°œê¸‰")
        print("\nì•„ë˜ ì •ë³´ë¥¼ ì…ë ¥í•˜ì„¸ìš”:")

        client_id = input("Client ID: ").strip()
        client_secret = input("Client Secret: ").strip()

        if client_id and client_secret:
            config["NAVER_BLOG_CLIENT_ID"] = client_id
            config["NAVER_BLOG_CLIENT_SECRET"] = client_secret
            save_config(config)
            collector = KeywordCollector(client_id, client_secret)
            print("\n[ì €ì¥ ì™„ë£Œ] API í‚¤ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        else:
            print("[ì˜¤ë¥˜] API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            sys.exit(1)

    print("\n" + "=" * 50)
    print("ë¸”ë¡œê·¸ í‚¤ì›Œë“œ ìˆ˜ì§‘ê¸° (ë„¤ì´ë²„ API)")
    print("=" * 50)

    keyword = input("\në¶„ì„í•  í‚¤ì›Œë“œ (ì‰¼í‘œë¡œ êµ¬ë¶„): ").strip()
    if not keyword:
        print("[ì˜¤ë¥˜] í‚¤ì›Œë“œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        sys.exit(1)

    keywords = [k.strip() for k in keyword.split(",") if k.strip()]

    max_count = input("ìµœëŒ€ ìˆ˜ì§‘ í‚¤ì›Œë“œ ìˆ˜ [ê¸°ë³¸: 50]: ").strip()
    max_count = int(max_count) if max_count.isdigit() else 50

    results = collector.collect_keywords(keywords, max_keywords=max_count)

    print("\n" + "=" * 50)
    print("ê³¨ë“  í‚¤ì›Œë“œ TOP 10")
    print("=" * 50)

    golden = [r for r in results if r["is_golden"]][:10]
    for i, r in enumerate(golden, 1):
        print(f"{i}. {r['keyword']} - ë¬¸ì„œ {r['docs']:,}ê°œ {r['rating']}")

    save_results(results, keywords[0])
